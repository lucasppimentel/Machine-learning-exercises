# Categorigal Cross-Entropy:
# Loss = -log(y'), where y is the certainty of the right classification

# Full function:
# Loss = -Sum(yi*log(y'i)), where yi is the target value of the class i
#and y'i is the predicted (certainty) of class i

